{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iowa Housing Prices Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Abstract\n",
    " This project utilizes Iowa's Housing dataset, which provides comprehensive information on residential properties in Ames and Des Moines, Iowa, to develop accurate predictive models for house prices. Since Des Moines is the largest populated city in Iowa, so I chose this dataset. Key features selected for analysis include \"Heating,\" \"HeatingQC,\" \"GrLivArea,\" \"SalePrice,\" \"BedroomAbvGr,\" \"KitchenQual,\" \"GarageCars,\" \"GarageQual,\" and \"WoodDeckSF.\" Categorical variables are converted to numerical values to facilitate exploratory data analysis (EDA) and model building. By drawing on previous research and methodologies, including Gusthema's Kaggle kernel and studies on housing price determinants by Deng et al. and DiPasquale and Wheaton, this project aims to enhance the understanding of housing price dynamics in Lowa. To enhance model performance, I conducted hyperparameter tuning and cross-validation. The final model achieved a high degree of accuracy and demonstrated robust predictive capabilities. The results of this study can assist real estate professionals, potential buyers, and policymakers in making informed decisions regarding property investments and pricing strategies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What factors have the greatest influence on predicting the sales price of residential properties, and how accurately can these factors be leveraged to predict SalePrice in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting housing prices is a crucial task in real estate, aiding buyers, sellers, and investors in making informed decisions. In the context of the Ames and Des Moines Housing dataset, which comprises extensive information on residential properties in Ames, Iowa, this project aims to develop accurate predictive models for house prices. The dataset includes diverse features such as dwelling types, zoning classifications, lot sizes, building styles, and various property attributes, offering a rich resource for exploring the determinants of housing prices in the region.\n",
    "\n",
    "Prior work in housing price prediction has explored similar datasets and methodologies to understand the factors influencing property values and develop predictive models. For instance, Gusthema's Kaggle kernel titled \"House Prices Prediction using TFDF\" provides an example of utilizing TensorFlow Decision Forests (TF-DF) to predict house prices based on the Ames Housing dataset. This work likely encompasses data preprocessing, feature engineering, and model training to achieve accurate predictions.\n",
    "\n",
    "Additionally, research in real estate economics has investigated the determinants of housing prices in various contexts. Studies like that by Deng et al. (2016) analyzed housing price determinants in Beijing, China, while DiPasquale and Wheaton (1996) examined the impact of neighborhood characteristics on property values in the United States. These studies highlight the importance of factors such as location, size, amenities, and neighborhood attributes in shaping housing prices.\n",
    "\n",
    "By synthesizing insights from prior work and leveraging the extensive features of the Ames Housing dataset, this project aims to contribute to the understanding of housing price dynamics in Ames, Iowa. Through exploratory data analysis, feature engineering, and advanced regression techniques, I seek to develop accurate predictive models for house prices while uncovering actionable insights for stakeholders in the local real estate market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sales price of residential properties is primarily influenced by factors such as location, size (square footage), number of bedrooms and bathrooms, overall condition, and amenities in Iowa, based on the data of Ames and Des Moines.\n",
    "\n",
    "The sales price of residential properties is predominantly influenced by location, size, condition, and amenities. Factors such as neighborhood desirability, square footage, overall condition, and the presence of amenities like bedrooms, bathrooms, and updated features play crucial roles in determining property value. Understanding and accurately assessing these factors are essential for predicting house prices effectively in the real estate market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data overview\n",
    "\n",
    "For each dataset include the following information\n",
    "- Dataset #1\n",
    "  - Dataset Name: Ames.csv\n",
    "  - Number of observations: 1320\n",
    "  - Number of variables: 15\n",
    "    \n",
    "\n",
    "- Dataset #2\n",
    "  - Dataset Name: Des Moines.csv\n",
    "  - Number of observations: 1610\n",
    "  - Number of variables: 15\n",
    " \n",
    "    \n",
    "In these dataset, I will include variables that like heating, different sizes of living area, numbers of different rooms, and also their conditions. Most of the data have numbers to it, like the size or the numbers, I also assgin the number to the conditions by dividing them into different levels. Moreover, there are variables that contains strings. This dataset is very close to out ideal dataset, but I will further choose the variables that I need later and also asign the different values to certain variables for easier analysis. \n",
    "\n",
    "These variables description are from the original dataset. Based on our normal knowledge and experiences, I choose some features first to do the analysis.\n",
    "\n",
    "These features include ['Foundation', 'GrLivArea', 'MSSubClass', 'TotalBsmtSF','YrSold','YearBuilt', 'BedroomAbvGr', 'OverallQual','OverallCond', 'Neighborhood', 'Functional', 'KitchenAbvGr', 'TotRmsAbvGrd', 'GarageCars']. In the future analysis and model building, I may also choose or delete some features.\n",
    "However some values are string such as \"Ex\", but I expect to have int values so that it will be convenient for us to do EDA and model building. \n",
    "\n",
    "Hence, I change these values to corresponding numbers. I change Ex to 5, Gd to 4, TA to 3, Fa to 2, Po to 1, NA to 0. And also replace Typ, Min1, Min2, Mod, Maj1, Maj2, Sev and Sal correlated to 0 to 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SalePrice', 'Foundation', 'GrLivArea', 'MSSubClass', 'TotalBsmtSF',\n",
       "       'YrSold', 'YearBuilt', 'BedroomAbvGr', 'OverallQual', 'OverallCond',\n",
       "       'Neighborhood', 'Functional', 'KitchenAbvGr', 'TotRmsAbvGrd',\n",
       "       'GarageCars', 'Yrowned'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "df_Ames = pd.read_csv('Ames.csv')\n",
    "df_Des_Moines = pd.read_csv('Des Moines.csv')\n",
    "df = pd.concat([df_Ames, df_Des_Moines], axis=0)\n",
    "lst = ['SalePrice','Foundation', 'GrLivArea', 'MSSubClass', 'TotalBsmtSF','YrSold','YearBuilt', 'BedroomAbvGr', 'OverallQual','OverallCond', 'Neighborhood', 'Functional', \n",
    "       'KitchenAbvGr', 'TotRmsAbvGrd', 'GarageCars']\n",
    "df = df[lst]\n",
    "df.isna().any()\n",
    "df.replace({'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po':1, 'NA':0}, inplace = True)\n",
    "df.replace({'Typ': 0, 'Min1': 1,'Min2': 2, 'Mod': 3, 'Maj1': 4, 'Maj2': 5, 'Sev': 6, 'Sal' : 7}, inplace = True)\n",
    "df.fillna(0, inplace = True)\n",
    "df['Yrowned'] = df['YrSold'] - df['YearBuilt']\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1 of EDA - Evaluation of Machine Learning Models for Predicting House Prices\n",
    "\n",
    "The provided code is designed to evaluate the performance of different machine learning models for predicting house prices based on a variety of features. It begins by importing necessary libraries for data manipulation and machine learning. It then prepares the feature set (`X`) and the target variable (`y`) from a DataFrame `df`, selecting relevant columns like 'GrLivArea', 'OverallQual', and 'Neighborhood'. The data is split into training and test sets with an 80-20 ratio. The core of the process is a loop that runs ten iterations of model training and evaluation. For each iteration, it splits the data, constructs a preprocessing pipeline that includes one-hot encoding for the 'Neighborhood' categorical variable, and combines this with a RandomForestRegressor model. The pipeline is then fitted to the training data, and R² scores are computed for both the training and test sets. These scores are collected across all iterations, and the average R² scores for training and test sets are calculated and printed, providing a measure of model performance and generalization capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F_onewayResult(statistic=144.3950774998117, pvalue=0.0)\n",
      "F_onewayResult(statistic=227.56782672988663, pvalue=1.2671240384616744e-205)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Performing ANOVA\n",
    "neighborhoods = df['Neighborhood'].unique()\n",
    "neighborhoods_prices = [df[df['Neighborhood'] == neighborhood]['SalePrice'] for neighborhood in neighborhoods]\n",
    "anova_result_np = f_oneway(*neighborhoods_prices)\n",
    "print(anova_result_np)\n",
    "\n",
    "foundations = df['Foundation'].unique()\n",
    "foundations_prices = [df[df['Foundation'] == foundation]['SalePrice'] for foundation in foundations]\n",
    "anova_result_fp = f_oneway(*foundations_prices)\n",
    "print(anova_result_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SalePrice', 'Foundation', 'GrLivArea', 'MSSubClass', 'TotalBsmtSF',\n",
       "       'YrSold', 'YearBuilt', 'BedroomAbvGr', 'OverallQual', 'OverallCond',\n",
       "       'Neighborhood', 'Functional', 'KitchenAbvGr', 'TotRmsAbvGrd',\n",
       "       'GarageCars', 'Yrowned'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X = df[['GrLivArea','MSSubClass', 'BedroomAbvGr', 'Yrowned', 'TotalBsmtSF','OverallQual','OverallCond', 'Neighborhood', 'Functional', 'KitchenAbvGr', 'TotRmsAbvGrd', 'GarageCars']]\n",
    "y = df['SalePrice']\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       127500\n",
       "1       149900\n",
       "2       120000\n",
       "3       146000\n",
       "4       376162\n",
       "         ...  \n",
       "1605    142500\n",
       "1606    131000\n",
       "1607    132000\n",
       "1608    170000\n",
       "1609    188000\n",
       "Name: SalePrice, Length: 2930, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "train_r2_scores = []\n",
    "test_r2_scores = []\n",
    "\n",
    "X = df[['GrLivArea','MSSubClass', 'BedroomAbvGr', 'Yrowned', 'TotalBsmtSF','OverallQual','OverallCond', 'Neighborhood', 'Functional', 'KitchenAbvGr', 'TotRmsAbvGrd', 'GarageCars']]\n",
    "y = df['SalePrice']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training R^2 score (RF) over 200 iterations: 0.982\n",
      "Average Test R^2 score (RF) over 200 iterations: 0.916\n",
      "Average Training R^2 score (GB) over 200 iterations: 0.954\n",
      "Average Test R^2 score (GB) over 200 iterations: 0.925\n",
      "Average Training R^2 score (LR) over 200 iterations: 0.834\n",
      "Average Test R^2 score (LR) over 200 iterations: 0.877\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X = df[['GrLivArea','MSSubClass', 'BedroomAbvGr', 'Yrowned', 'TotalBsmtSF','OverallQual','OverallCond', 'Neighborhood', 'Functional', 'KitchenAbvGr', 'TotRmsAbvGrd', 'GarageCars']]\n",
    "y = df['SalePrice']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "# Loop 200 times\n",
    "for _ in range(200):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('onehot', OneHotEncoder(), ['Neighborhood'])\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    pipeline_GB = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=3))\n",
    "    ])\n",
    "\n",
    "    pipeline_LR = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    pipeline_RF = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100))\n",
    "    ])\n",
    "\n",
    "    train_r2_scores_RF = []\n",
    "    test_r2_scores_RF = []\n",
    "\n",
    "    pipeline_RF.fit(X_train, y_train)\n",
    "    train_score_RF = pipeline_RF.score(X_train, y_train)\n",
    "    test_score_RF = pipeline_RF.score(X_test, y_test)\n",
    "    train_r2_scores_RF.append(train_score_RF)\n",
    "    test_r2_scores_RF.append(test_score_RF)\n",
    "\n",
    "    train_r2_scores_GB = []\n",
    "    test_r2_scores_GB = []\n",
    "\n",
    "    pipeline_GB.fit(X_train, y_train)\n",
    "    train_score_GB = pipeline_GB.score(X_train, y_train)\n",
    "    test_score_GB = pipeline_GB.score(X_test, y_test)\n",
    "    train_r2_scores_GB.append(train_score_GB)\n",
    "    test_r2_scores_GB.append(test_score_GB)\n",
    "\n",
    "    train_r2_scores_LR = []\n",
    "    test_r2_scores_LR = []\n",
    "\n",
    "    pipeline_LR.fit(X_train, y_train)\n",
    "    train_score_LR = pipeline_LR.score(X_train, y_train)\n",
    "    test_score_LR = pipeline_LR.score(X_test, y_test)\n",
    "    train_r2_scores_LR.append(train_score_LR)\n",
    "    test_r2_scores_LR.append(test_score_LR)\n",
    "\n",
    "# Compute the average R^2 scores\n",
    "avg_train_r2_RF = np.mean(train_r2_scores_RF)\n",
    "avg_test_r2_RF = np.mean(test_r2_scores_RF)\n",
    "\n",
    "avg_train_r2_GB = np.mean(train_r2_scores_GB)\n",
    "avg_test_r2_GB = np.mean(test_r2_scores_GB)\n",
    "\n",
    "avg_train_r2_LR = np.mean(train_r2_scores_LR)\n",
    "avg_test_r2_LR = np.mean(test_r2_scores_LR)\n",
    "\n",
    "print(f\"Average Training R^2 score (RF) over 200 iterations: {avg_train_r2_RF:.3f}\")\n",
    "print(f\"Average Test R^2 score (RF) over 200 iterations: {avg_test_r2_RF:.3f}\")\n",
    "print(f\"Average Training R^2 score (GB) over 200 iterations: {avg_train_r2_GB:.3f}\")\n",
    "print(f\"Average Test R^2 score (GB) over 200 iterations: {avg_test_r2_GB:.3f}\")\n",
    "print(f\"Average Training R^2 score (LR) over 200 iterations: {avg_train_r2_LR:.3f}\")\n",
    "print(f\"Average Test R^2 score (LR) over 200 iterations: {avg_test_r2_LR:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discusison and Conclusion\n",
    "\n",
    "In the course of our analysis, I explored multiple regression techniques, including Linear Regression and Gradient Boosting Regression, alongside the Random Forest Regressor. Despite their potential applicability, these models did not yield satisfactory results in terms of predictive accuracy, as indicated by relatively low R² scores. Consequently, they were not selected for further consideration in our modeling approach. This iterative process of experimentation and evaluation underscores the importance of selecting appropriate modeling techniques that align with the complexity and nuances of the housing price prediction task.\n",
    "\n",
    "The decision to exclude Linear Regression and Random Forest Regressor models was driven by the pursuit of models that could effectively capture the non-linear relationships and interactions among the numerous features influencing housing prices. While Linear Regression assumes a linear relationship between the independent and dependent variables, the housing market often exhibits non-linear behavior due to the diverse array of factors influencing property values. Similarly, Random Forest Regressor, although capable of capturing non-linear relationships, may have struggled to adequately represent the complex interactions among features without extensive hyperparameter tuning and feature engineering. The linear model are assuming linear relationship between features and the sale price variable, while from the data visualization, some of the feature I are not sure they are guaranteed linear relationship while Gradient Boosting inherently handle non-linear relationships between features and the sale price by using multiple decision trees. Each tree is built on a random subset of the data and features, which allows the ensemble to capture complex, non-linear patterns. Compare to Gradient Boosting Regression, Random Forests are less likely to overfit compared to a single decision tree. This makes them more robust on unseen data, assuming the number of trees is sufficient and not excessively high.\n",
    "\n",
    "Despite their exclusion from the final model selection, the exploration of Linear Regression and Gradient Boosting Regression provided valuable insights into the limitations of certain modeling approaches in the context of housing price prediction. By systematically evaluating the performance of different algorithms, I gained a deeper understanding of the strengths and weaknesses inherent in each method. This iterative process of model selection and refinement highlights the importance of rigorously assessing various modeling techniques to identify the most suitable approach for a given task. The linear model are assuming linear relationship between features and the sale price variable, while from the data visualization, some of the feature I are not sure they are guaranteed linear relationship while Random Forests is less prone to overfitting because it averages the results of many trees, which individually might overfit.  Gradient Boosting on other hand builds one tree at a time where each new tree helps to correct errors made by previously built trees. This sequential correction of residuals can lead to a highly optimized predictor.\n",
    "\n",
    "Moreover, based on the R^2 score performed above, since Gradient Boosting Regression has the highest score: 0.926 performed, higher than two other testing methods. Random Forest has the score: 0.916 and Linear Regression has: 0.877. In this way, I decided to choose to use Gradient Boosting Regression. \n",
    "\n",
    "In conclusion, while Linear Regression and Random Forest were considered during the modeling process, they ultimately did not meet the criteria for predictive accuracy required for our analysis. The decision to focus on the Gradient Boosting Regression was based on its superior performance in capturing the complex relationships within the housing market data. Moving forward, continued exploration of alternative modeling techniques and robust evaluation methods will be essential for further refining our predictive models and enhancing our understanding of housing price dynamics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
